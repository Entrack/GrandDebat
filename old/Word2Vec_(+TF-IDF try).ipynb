{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Black\n",
      "[nltk_data]     Overlord\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# для того, чтобы увеличить максииальный объём файла, который можно загружать в память\n",
    "import sys\n",
    "import csv\n",
    "def find_and_set_sys_maxsize():\n",
    "    maxInt = sys.maxsize\n",
    "    decrement = True\n",
    "    while decrement:\n",
    "        decrement = False\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "            decrement = True\n",
    "find_and_set_sys_maxsize()\n",
    "\n",
    "# отключим ворнинги\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "# штука для предобработки текста\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "import datetime\n",
    "def timestamp(): return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "granddebat_folder = '../GrandDébat/granddebat.fr/'\n",
    "democrat_path = os.path.join(granddebat_folder, 'DEMOCRATIE_ET_CITOYENNETE.csv')\n",
    "events_path = os.path.join(granddebat_folder, 'EVENTS.csv')\n",
    "fiscalit_path = os.path.join(granddebat_folder, 'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv')\n",
    "ecolog_path = os.path.join(granddebat_folder, 'LA_TRANSITION_ECOLOGIQUE.csv')\n",
    "organisat_path = os.path.join(granddebat_folder, 'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv')\n",
    "\n",
    "french_encoding = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = democrat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_raw = pd.read_csv(file_path, engine='python', encoding=french_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>Ew</th>\n",
       "      <th>Ex</th>\n",
       "      <th>Ey</th>\n",
       "      <th>Ez</th>\n",
       "      <th>E0</th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>...</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>Qx</th>\n",
       "      <th>Qy</th>\n",
       "      <th>Qz</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le citoyen</td>\n",
       "      <td>Non</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afin d’éviter de creuser les inégalités ne plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un instrument de démocratie locale à modernise...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nous proposons le retour à la limitation de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voir l'intégralité de la proposition dans la d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POUR UN NOUVEAU CONTRAT CITOYEN               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>député, maire, moi même</td>\n",
       "      <td>Non</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Budget participatif, possibilité d'interpeller...</td>\n",
       "      <td>Une bonne chose</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  A3   A4   A5  \\\n",
       "0                                         Le citoyen  Non  NaN   \n",
       "1  Un instrument de démocratie locale à modernise...  NaN  NaN   \n",
       "2                                                NaN  NaN  NaN   \n",
       "3  Voir l'intégralité de la proposition dans la d...  NaN  NaN   \n",
       "4                            député, maire, moi même  Non  NaN   \n",
       "\n",
       "                                                  Ew               Ex   Ey  \\\n",
       "0                                                NaN              NaN  NaN   \n",
       "1                                                NaN              NaN  NaN   \n",
       "2                                                NaN              NaN  NaN   \n",
       "3                                                NaN              NaN  NaN   \n",
       "4  Budget participatif, possibilité d'interpeller...  Une bonne chose  NaN   \n",
       "\n",
       "    Ez   E0   E1   E2                        ...                           M1  \\\n",
       "0  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "1  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "2  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "3  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "4  NaN  Oui  NaN  NaN                        ...                          NaN   \n",
       "\n",
       "    M2   M3   M4   M5   Qx   Qy   Qz   Q0  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                                                  Q1  \n",
       "0  Afin d’éviter de creuser les inégalités ne plu...  \n",
       "1                                                NaN  \n",
       "2  Nous proposons le retour à la limitation de vi...  \n",
       "3  POUR UN NOUVEAU CONTRAT CITOYEN               ...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_reverse_index = 37\n",
    "db_questions = db_raw.loc[:, db_raw.columns[-questions_reverse_index:]]\n",
    "\n",
    "# (QA4893whateverA3, QA4893whateverA4) -> (A3, A4)\n",
    "question_list = db_raw.columns[-questions_reverse_index:]\n",
    "question_list = [el.split()[0][-2:] for el in question_list]\n",
    "db_questions.columns = question_list\n",
    "\n",
    "db_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names_to_zipCode_regions = {\n",
    "\"Île de France\": [\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"],\n",
    "\"Nord-Pas-de-Calais Picardie\": [\"02\", \"59\", \"60\", \"62\", \"80\"],\n",
    "\"Alsace Champagne-Ardenne Lorraine\": [\"08\", \"10\", \"51\", \"52\", \"54\", \"55\", \"57\", \"67\", \"88\", \"68\"],\n",
    "\"Normandie\": [\"14\", \"27\", \"50\", \"61\", \"76\"],\n",
    "\"Bretagne\": [\"22\", \"29\", \"35\", \"56\"],\n",
    "\"Pays de la Loire\": [\"44\", \"49\", \"53\", \"72\", \"85\"],\n",
    "\"Centre Val de Loire\": [\"18\", \"28\", \"36\", \"37\", \"41\", \"45\"],\n",
    "\"Bourgogne Franche-Comté\": [\"21\", \"25\", \"39\", \"58\", \"70\", \"71\", \"89\", \"90\"],\n",
    "\"Aquitaine Limousin Poitou-Charentes\": [\"16\", \"17\", \"19\", \"23\", \"24\", \"33\", \"40\", \"47\", \"64\", \"79\", \"86\", \"87\"],\n",
    "\"Auvergne-Rhône-Alpes\": [\"01\", \"03\", \"07\", \"15\", \"26\", \"38\", \"42\", \"43\", \"63\", \"69\", \"73\", \"74\"],\n",
    "\"Languedoc-Roussillon Midi Pyrénées\": [\"09\", \"11\", \"12\", \"30\", \"31\", \"32\", \"34\", \"46\", \"48\", \"65\", \"66\", \"81\", \"82\"],\n",
    "\"Provence Alpes Côte d'Azur\": [\"04\", \"05\", \"06\", \"13\", \"83\", \"84\"],\n",
    "\"Corse\": [\"20\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \" \".join(gensim.utils.simple_preprocess(text))\n",
    "    return text\n",
    "\n",
    "def strip_zipCode(zipCode):\n",
    "    return str(zipCode)[0:2]\n",
    "\n",
    "def get_region_by_stripped_zipCode(zipCode):\n",
    "    region_index = None\n",
    "    for idx, region in enumerate(region_names_to_zipCode_regions):\n",
    "        if zipCode in region_names_to_zipCode_regions[region]:\n",
    "            region_index = idx\n",
    "    return region_index\n",
    "\n",
    "def preprocess_zipCode(zipCode):\n",
    "    zipCode = strip_zipCode(zipCode)\n",
    "    return get_region_by_stripped_zipCode(zipCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 / 65758\n",
      "row 10000 / 65758\n",
      "row 20000 / 65758\n",
      "row 30000 / 65758\n",
      "row 40000 / 65758\n",
      "row 50000 / 65758\n",
      "row 60000 / 65758\n",
      "zipCodes that does not belong to any region: {'9', '96', '97', '4', '7', '99', '5', '1', '6', '0', '98', '3'}\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "db = db_questions\n",
    "\n",
    "texts = []\n",
    "authorZipCode_regions = []\n",
    "# question_numbers = []\n",
    "i = 0\n",
    "invalid_zipcodes = []\n",
    "\n",
    "for row in range(db.shape[0]):\n",
    "    if row % 10000 == 0: print(\"row\", row, \"/\", db.shape[0])\n",
    "    for col in range(db.shape[1]):\n",
    "        if not pd.isnull(db.iat[row, col]):\n",
    "            text = preprocess_text(db.iat[row, col])\n",
    "            authorZipCode = db_raw.at[row, 'authorZipCode']\n",
    "            authorZipCode_region = preprocess_zipCode(authorZipCode)\n",
    "#             question_number = col\n",
    "            \n",
    "            if authorZipCode_region == None:\n",
    "                invalid_zipcodes.append(strip_zipCode(authorZipCode))\n",
    "                authorZipCode_region = -1\n",
    "\n",
    "            texts.append(text)\n",
    "            authorZipCode_regions.append(authorZipCode_region)\n",
    "#             question_numbers.append(question_number)\n",
    "\n",
    "invalid_zipcodes = set(invalid_zipcodes)\n",
    "print(\"zipCodes that does not belong to any region:\", invalid_zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, zipCode in enumerate(authorZipCode_regions):\n",
    "    if zipCode == -1:\n",
    "        del texts[idx]\n",
    "        del authorZipCode_regions[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On this step we have a [texts] and [authorZipCode_regions] lists\n",
    "The first one has strings of answers as elements  \n",
    "The second one has one of ~14 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_text_with_same_tag(texts, tags):\n",
    "    new_texts = []\n",
    "    new_tags = []\n",
    "    tag_to_text = {}\n",
    "    for tag in set(tags):\n",
    "        tag_to_text[tag] = []\n",
    "    \n",
    "    for idx, tag in enumerate(tags):\n",
    "        if idx % (len(tags) // 10) == 0: print(\"%.0f%% done\" % (idx / len(tags) * 100))\n",
    "        tag_to_text[tag].append(texts[idx])\n",
    "    for tag in tag_to_text:\n",
    "        tag_to_text[tag] = \" \".join(tag_to_text[tag])\n",
    "    for tag in tag_to_text:\n",
    "        new_texts.append(tag_to_text[tag])\n",
    "        new_tags.append(tag)\n",
    "    return new_texts, new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done\n",
      "10% done\n",
      "20% done\n",
      "30% done\n",
      "40% done\n",
      "50% done\n",
      "60% done\n",
      "70% done\n",
      "80% done\n",
      "90% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "all_region_texts, regions = join_text_with_same_tag(texts, authorZipCode_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_region_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, -1]\n"
     ]
    }
   ],
   "source": [
    "print(regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On this step we have a [all_region_texts] and [regions] lists\n",
    "The first one has string of all the answers concatenacted for one region as the element\n",
    "The second one has a label for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = all_region_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(max_df=0.25)\n",
    "word_count_vector=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On this step what we already have is: we have counted the frequency of words in each region. We just have the words-region frequency matrix now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The region index (corpus_idx). If you want to see the most important words of the region 1, do corpus_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_idx = 1\n",
    "answer = corpus[corpus_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_to_weights_for_region_dict(answer, vocabulary_limit = 10):\n",
    "    words_to_weights_for_region_dict = {}\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([answer]))\n",
    "    print(tf_idf_vector)\n",
    "    feature_names=cv.get_feature_names()\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items, vocabulary_limit)\n",
    "    \n",
    "\n",
    "    for idx, el in enumerate(keywords):\n",
    "        keywords[el] = keywords[el] / (idx + 10)\n",
    "\n",
    "    norm = keywords[list(keywords)[0]]\n",
    "    for idx, el in enumerate(keywords):\n",
    "        keywords[el] /= norm\n",
    "        words_to_weights_for_region_dict[el] = keywords[el]\n",
    "    return words_to_weights_for_region_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The problem is that tfidf was trained using large large string as a corpus  \n",
    "#### But now it does not want to vectorize sentences less than N in length, so WE DON'T USE TF-IDF WEIGHTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 79560)\t0.2520009270350179\n",
      "  (0, 54857)\t0.3272343900309138\n",
      "  (0, 38897)\t0.3272343900309138\n",
      "  (0, 35695)\t0.283225635378922\n",
      "  (0, 32533)\t0.3272343900309138\n",
      "  (0, 31865)\t0.283225635378922\n",
      "  (0, 31864)\t0.283225635378922\n",
      "  (0, 31614)\t0.283225635378922\n",
      "  (0, 28416)\t0.3272343900309138\n",
      "  (0, 18568)\t0.3272343900309138\n",
      "  (0, 11249)\t0.283225635378922\n",
      "{'pléthoriennes': 1.0, 'inititative': 0.9090909090909091, 'frime': 0.8333333333333334, 'espacce': 0.7692307692307693, 'defiscalisant': 0.7142857142857143, 'histogrammes': 0.5769622833843017, 'fonderies': 0.5409021406727829, 'fonderie': 0.5090843676920309, 'flu': 0.4808019028202514, 'certianes': 0.4554965395139223}\n"
     ]
    }
   ],
   "source": [
    "print(get_words_to_weights_for_region_dict(answer[:20000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9eef7d72ff3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_words_to_weights_for_region_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-cb4a3975f3aa>\u001b[0m in \u001b[0;36mget_words_to_weights_for_region_dict\u001b[1;34m(answer, vocabulary_limit)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mkeywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mkeywords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(get_words_to_weights_for_region_dict(answer[:2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So what we have already, is the most important words for the region with idx == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have answers in X, regions in y  \n",
    "### We have weights that will say which words are more important in the answer  \n",
    "### So let's get the vector for each word of the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"./pretrained_w2v/fr.bin\")\n",
    "# https://github.com/Kyubyong/wordvectors\n",
    "# French (w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can have some fun with the pre-trained w2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biotope', 0.6826141476631165),\n",
       " ('environnement', 0.6822277903556824),\n",
       " ('aquifère', 0.6639423966407776),\n",
       " (\"l'écosystème\", 0.6581449508666992),\n",
       " ('biome', 0.6514424085617065),\n",
       " ('microclimat', 0.6483206152915955),\n",
       " ('habitat', 0.6472629904747009),\n",
       " ('humus', 0.616238534450531),\n",
       " ('écosystèmes', 0.597100019454956),\n",
       " ('anticyclone', 0.5882144570350647)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('écosystème')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can get the vector of the word (10 / 300 components displayed here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.6136349 ,  1.5202744 , -1.5171983 ,  1.0266268 , -0.12849323,\n",
       "        0.8902757 ,  0.47812438, -0.56151843, -1.2719674 ,  0.8245529 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model['écosystème'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we convert an individual answer sentences to vectors using word2vec pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'le citoyen'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2vec_texts_vectors = np.array((len(texts), 1))\n",
    "# unknown_to_pretrained_word_list = []\n",
    "# for idx, text in enumerate(texts):\n",
    "#     final_idx = len(texts)\n",
    "#     if idx % (final_idx // 10) == 0: print(\"%.0f%% done\" % (idx / final_idx * 100))\n",
    "#     word_vectors = []\n",
    "# #     word_weights = []\n",
    "# #     text_words_weights = get_words_to_weights_for_region_dict(text)\n",
    "#     for word in text.split():\n",
    "# #         if word in text_words_weights:\n",
    "#         try:\n",
    "#             word_vectors.append(w2v_model[word])\n",
    "#         except:\n",
    "#             if not word in unknown_to_pretrained_word_list:\n",
    "#                 unknown_to_pretrained_word_list.append(word)\n",
    "# #             word_weights.append(text_words_weights[word])\n",
    "#     if not len(word_vectors) == 0:\n",
    "#         word_vectors = np.array(word_vectors)\n",
    "#     else:\n",
    "#         word_vectors = np.zeros((300,))\n",
    "# #     word_weights = np.array(word_weights)\n",
    "# #     word_weighted_vectors = word_vectors * word_weights[:, np.newaxis]\n",
    "# #     word_combined_vector = word_weighted_vectors.sum(axis = 0) / word_weighted_vectors.shape[0]\n",
    "#     words_combined_vector = word_vectors.sum(axis = 0) / word_vectors.shape[0]\n",
    "# #     w2vec_texts_vectors.append(words_combined_vector)\n",
    "#     w2vec_texts_vectors.append(words_combined_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The procedure below just goes through all the answers and averages w2vec vectors for answer sentence (it is not so bad, since the dimensionality of pre-trained model is very high, so much of the information will be conserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10% done\n",
      "20% done\n",
      "30% done\n",
      "40% done\n",
      "50% done\n",
      "60% done\n",
      "70% done\n",
      "80% done\n",
      "90% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "w2vec_texts_vectors = []\n",
    "unknown_to_pretrained_word_list = []\n",
    "for idx, text in enumerate(texts):\n",
    "    final_idx = len(texts)\n",
    "    if idx % (final_idx // 10) == 0: print(\"%.0f%% done\" % (idx / final_idx * 100))\n",
    "    word_vectors = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            word_vectors.append(w2v_model[word])\n",
    "        except:\n",
    "            if not word in unknown_to_pretrained_word_list:\n",
    "                unknown_to_pretrained_word_list.append(word)\n",
    "    if not len(word_vectors) == 0:\n",
    "        word_vectors = np.array(word_vectors, dtype=np.float16)\n",
    "        words_combined_vector = word_vectors.sum(axis = 0) / word_vectors.shape[0]\n",
    "    else:\n",
    "        words_combined_vector = np.zeros((300,), dtype=np.float16)\n",
    "    w2vec_texts_vectors.append(words_combined_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_to_pretrained_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_texts_vectors_np = np.array(w2vec_texts_vectors, dtype=np.float16)#.reshape(-1, 1)\n",
    "# w2vec_texts_vectors_np = np.vstack(w2vec_texts_vectors)#.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_texts_vectors_np -= w2vec_texts_vectors_np.min()\n",
    "w2vec_texts_vectors_np /= w2vec_texts_vectors_np.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1590723, 300)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_texts_vectors_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For now, we've got the list w2vec_texts_vectors that has a vector of each answer sentence as it's element  \n",
    "#### Now we can fit it to the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = w2vec_texts_vectors_np\n",
    "y_train = authorZipCode_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "# param['silent'] = 1\n",
    "# param['verbosity'] = 2\n",
    "param['nthread'] = 4\n",
    "\n",
    "param['max_depth'] = 6\n",
    "num_boost_round = 10\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_boost_round = num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_X_test = xgb.DMatrix(X_test)\n",
    "xgb_y_test = xgb.DMatrix(y_test)\n",
    "\n",
    "y_pred = bst.predict(xgb_X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "random_accuracy = 1 / len(set(authorZipCode_regions))\n",
    "print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-f6e5892eec8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# fit_prior = False decreases acc by 0.1% which means that we have a slight imbalance between zipcodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[0;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input X must be non-negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "\n",
    "# fit_prior = False decreases acc by 0.1% which means that we have a slight imbalance between zipcodes\n",
    "nb_model = NB().fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go buy some RAM :>\n",
    "#### I get memory errors on both of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "random_accuracy = 1 / len(set(authorZipCode_regions))\n",
    "print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
