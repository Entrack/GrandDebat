{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Black\n",
      "[nltk_data]     Overlord\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"machine learning\",\n",
    "        \"coding in python\",\n",
    "        \"building chatbots\",\n",
    "        \"cats and puppies\",\n",
    "        \"nip of the cat and doggos\",\n",
    "        \"doggos and puppies\",\n",
    "       ]\n",
    "\n",
    "\n",
    "tags = [0, 0, 0, 1, 1, 1]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[tags[i]]) for i, _d in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['machine', 'learning'], tags=[0]),\n",
       " TaggedDocument(words=['coding', 'in', 'python'], tags=[0]),\n",
       " TaggedDocument(words=['building', 'chatbots'], tags=[0]),\n",
       " TaggedDocument(words=['cats', 'and', 'puppies'], tags=[1]),\n",
       " TaggedDocument(words=['nip', 'of', 'the', 'cat', 'and', 'doggos'], tags=[1]),\n",
       " TaggedDocument(words=['doggos', 'and', 'puppies'], tags=[1])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "#     print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9900795\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "# print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "# similar_doc = model.docvecs.most_similar(1)\n",
    "print(model.docvecs.similarity(0, 1))\n",
    "# print(similar_doc)\n",
    "# print(similar_doc['1'] - similar_doc['2'])\n",
    "# print(similar_doc['1'] - similar_doc['3'])\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "# print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8942561"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity(\"python\", \"i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          e         f         g         h\n",
      "a -1.306788  0.060931  0.892931 -1.625346\n",
      "b -0.634520  2.433163 -0.136470 -1.278625\n",
      "c  1.187531  1.799701  0.212438  0.132352\n",
      "d  0.717646 -1.440838 -0.763843  1.553971\n",
      "[('a', 'f'), ('a', 'g'), ('b', 'f'), ('c', 'e'), ('c', 'f'), ('c', 'g'), ('c', 'h'), ('d', 'e'), ('d', 'h')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "x = pd.DataFrame(np.random.normal(0, 1, (4,4)), index=['a', 'b', 'c', 'd'],\n",
    "                 columns=['e', 'f', 'g', 'h'])\n",
    "indices = list(x[x > 0].stack().index)\n",
    "print(x)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3701034811734984\n",
      "1.3923996398290475\n",
      "0.7397749593259009\n",
      "0.27097624064396975\n",
      "0.6216205930243441\n",
      "0.3990125082206994\n",
      "2.272872502063637\n",
      "0.24109693561652243\n"
     ]
    }
   ],
   "source": [
    "for idx in indices:\n",
    "    print(x.loc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3701034811734984"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.loc[indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['oui',\n",
       " 'mais',\n",
       " 'actuellement',\n",
       " 'pas',\n",
       " 'les',\n",
       " 'syndicats',\n",
       " 'trop',\n",
       " 'politisés',\n",
       " 'et',\n",
       " 'qui',\n",
       " 'ont',\n",
       " 'ignoré',\n",
       " 'au',\n",
       " 'détriment',\n",
       " 'des',\n",
       " 'départements']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gensim.utils.simple_preprocess(\"Professionnelles oui mais actuellement pas les syndicats trop politisés et qui ont ignoré au détriment des départements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 499 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['professionnelles',\n",
       " 'oui',\n",
       " 'mais',\n",
       " 'actuellement',\n",
       " 'pas',\n",
       " 'les',\n",
       " 'syndicats',\n",
       " 'trop',\n",
       " 'politisés',\n",
       " 'et',\n",
       " 'qui',\n",
       " 'ont',\n",
       " 'ignoré',\n",
       " 'au',\n",
       " 'détriment',\n",
       " 'des',\n",
       " 'départements',\n",
       " '.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "word_tokenize(\"Professionnelles oui mais actuellement pas les syndicats trop politisés et qui ont ignoré au détriment des départements.\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"./pretrained_w2v/fr.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('biotope', 0.6826141476631165),\n",
       " ('environnement', 0.6822277903556824),\n",
       " ('aquifère', 0.6639423966407776),\n",
       " (\"l'écosystème\", 0.6581449508666992),\n",
       " ('biome', 0.6514424085617065),\n",
       " ('microclimat', 0.6483206152915955),\n",
       " ('habitat', 0.6472629904747009),\n",
       " ('humus', 0.616238534450531),\n",
       " ('écosystèmes', 0.597100019454956),\n",
       " ('anticyclone', 0.5882144570350647)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('écosystème')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"A tout élu au suffrage universel, avec des règles à revoir.\",\n",
    "    \"Aux syndicats, s'ils se réforment. Aux associations citoyennes à créer. Il faut des organes permettant des débats annuels , à l'image de ce débat citoyen\",\n",
    "    \"Que constatons-nous ? Que malgré leurs excès, malgré leur tempérament patient mais explosif une fois cette patience mise à \",\n",
    "    \"mal, les Français sont des gens raisonnables.  Que demandent-ils au fond, au-delà des questions pourtant essentielles de pouvoir d’achat et de bien être personnel ? \",\n",
    "    \"Ils demandent un cadre, clair, et une fois pour toute garant des fondements de la démocratie « à la française » : liberté, égalité, fraternité mais aussi laïcité, équité et éthique.  Aussi, en ce qui concerne le débat national sur la démocratie et sa pratique , plusieurs choses simples seraient à mettre en œuvre :\",\n",
    "    \"plusieurs avantages : donner satisfaction aux citoyens mécontents ET améliorer les conditions d’exercice de la démocratie.\",\n",
    "    \"Il faut éviter que politique devienne un métier ou une profession. il faut donc aussi renforcer le rôle de l'élu et son statut tout en garantissant son retour à la vie civile\",\n",
    "    \"Elle est indispensable. Il faut réfléchir à un système évitant l'entre soi : il est presque impossible financièrement,\",\n",
    "    \"hors d'un parti, de se présenter à une élection avec une audience équitable\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(max_df=0.25)\n",
    "word_count_vector=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1\n",
    "\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([corpus[idx]]))\n",
    "feature_names=cv.get_feature_names()\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "équitable 1.0\n",
      "élection 0.7788007830714049\n",
      "présenter 0.6065306597126334\n",
      "parti 0.4723665527410147\n",
      "hors 0.3678794411714424\n",
      "audience 0.2865047968601901\n",
      "se 0.18847506715534945\n",
      "avec 0.14678452989002178\n"
     ]
    }
   ],
   "source": [
    "for idx, el in enumerate(keywords):\n",
    "#     keywords[el] = keywords[el] ** 2\n",
    "#     keywords[el] = keywords[el] / (idx / 6 + 1)\n",
    "    keywords[el] = keywords[el] / (math.exp(idx / 4))\n",
    "    \n",
    "norm = keywords[list(keywords)[0]]\n",
    "for idx, el in enumerate(keywords):\n",
    "    keywords[el] /= norm\n",
    "    print(el, keywords[el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join all answers in index in all questions -- о чём вообще говорит этот регион\n",
    "join all answers in index in one question -- о чём говорит этот регион по этому вопросу"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
