{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для того, чтобы увеличить максииальный объём файла, который можно загружать в память\n",
    "import sys\n",
    "import csv\n",
    "def find_and_set_sys_maxsize():\n",
    "    maxInt = sys.maxsize\n",
    "    decrement = True\n",
    "    while decrement:\n",
    "        decrement = False\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "            decrement = True\n",
    "find_and_set_sys_maxsize()\n",
    "\n",
    "# отключим ворнинги\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "# штука для предобработки текста\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "import datetime\n",
    "def timestamp(): return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Data preparation _ Baseline file for some data preprocessing explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def print_progress_bar(iteration, total, prefix = '', suffix = '', decimals = 0):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    length = 50\n",
    "    fill = '='\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    progress_bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    print('\\r%s [%s] %s%% %s' % (prefix, progress_bar, percent, suffix), end='\\r')\n",
    "    # Print new line on complete\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "granddebat_folder = '../GrandDébat/granddebat.fr/'\n",
    "democrat_path = os.path.join(granddebat_folder, 'DEMOCRATIE_ET_CITOYENNETE.csv')\n",
    "events_path = os.path.join(granddebat_folder, 'EVENTS.csv')\n",
    "fiscalit_path = os.path.join(granddebat_folder, 'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv')\n",
    "ecolog_path = os.path.join(granddebat_folder, 'LA_TRANSITION_ECOLOGIQUE.csv')\n",
    "organisat_path = os.path.join(granddebat_folder, 'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv')\n",
    "\n",
    "french_encoding = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = democrat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_raw = pd.read_csv(file_path, engine='python', encoding=french_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>Ew</th>\n",
       "      <th>Ex</th>\n",
       "      <th>Ey</th>\n",
       "      <th>Ez</th>\n",
       "      <th>E0</th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>...</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>Qx</th>\n",
       "      <th>Qy</th>\n",
       "      <th>Qz</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le citoyen</td>\n",
       "      <td>Non</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afin d’éviter de creuser les inégalités ne plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un instrument de démocratie locale à modernise...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nous proposons le retour à la limitation de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voir l'intégralité de la proposition dans la d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POUR UN NOUVEAU CONTRAT CITOYEN               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>député, maire, moi même</td>\n",
       "      <td>Non</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Budget participatif, possibilité d'interpeller...</td>\n",
       "      <td>Une bonne chose</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  A3   A4   A5  \\\n",
       "0                                         Le citoyen  Non  NaN   \n",
       "1  Un instrument de démocratie locale à modernise...  NaN  NaN   \n",
       "2                                                NaN  NaN  NaN   \n",
       "3  Voir l'intégralité de la proposition dans la d...  NaN  NaN   \n",
       "4                            député, maire, moi même  Non  NaN   \n",
       "\n",
       "                                                  Ew               Ex   Ey  \\\n",
       "0                                                NaN              NaN  NaN   \n",
       "1                                                NaN              NaN  NaN   \n",
       "2                                                NaN              NaN  NaN   \n",
       "3                                                NaN              NaN  NaN   \n",
       "4  Budget participatif, possibilité d'interpeller...  Une bonne chose  NaN   \n",
       "\n",
       "    Ez   E0   E1   E2                        ...                           M1  \\\n",
       "0  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "1  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "2  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "3  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "4  NaN  Oui  NaN  NaN                        ...                          NaN   \n",
       "\n",
       "    M2   M3   M4   M5   Qx   Qy   Qz   Q0  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                                                  Q1  \n",
       "0  Afin d’éviter de creuser les inégalités ne plu...  \n",
       "1                                                NaN  \n",
       "2  Nous proposons le retour à la limitation de vi...  \n",
       "3  POUR UN NOUVEAU CONTRAT CITOYEN               ...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_reverse_index = 37\n",
    "db_questions = db_raw.loc[:, db_raw.columns[-questions_reverse_index:]]\n",
    "\n",
    "question_list = db_raw.columns[-questions_reverse_index:]\n",
    "question_list = [el.split()[0][-2:] for el in question_list]\n",
    "db_questions.columns = question_list\n",
    "\n",
    "db_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulated by postal regions (100 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \" \".join(gensim.utils.simple_preprocess(text))\n",
    "    return text\n",
    "\n",
    "def preprocess_zipCode(zipCode):\n",
    "    return int(str(zipCode)[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the postal region and answer text out of the dataframe, taking only first two digits as meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 / 65758\n",
      "row 10000 / 65758\n",
      "row 20000 / 65758\n",
      "row 30000 / 65758\n",
      "row 40000 / 65758\n",
      "row 50000 / 65758\n",
      "row 60000 / 65758\n"
     ]
    }
   ],
   "source": [
    "db = db_questions\n",
    "\n",
    "texts = []\n",
    "authorZipCode_regions = []\n",
    "question_numbers = []\n",
    "i = 0\n",
    "\n",
    "for row in range(db.shape[0]):\n",
    "    if row % 10000 == 0: print(\"row\", row, \"/\", db.shape[0])\n",
    "    for col in range(db.shape[1]):\n",
    "        if not pd.isnull(db.iat[row, col]):\n",
    "            text = preprocess_text(db.iat[row, col])\n",
    "            authorZipCode_region = preprocess_zipCode(db_raw.at[row, 'authorZipCode'])\n",
    "            question_number = col\n",
    "\n",
    "            texts.append(text)\n",
    "            authorZipCode_regions.append(authorZipCode_region)\n",
    "            question_numbers.append(question_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "This is a pretty powerful thus non computationally expensive method of analyzing texts  \n",
    "It counts the number of word occurences in the documents  \n",
    "Based on this, we can guess how important are some words for the document (how frequent they are in the document divided by how frequent they are in all the documents)  \n",
    "\n",
    "train_tfidf_texts is a sparse matrix that has mostly zeros  \n",
    "rows represent documets (answers here) and columns words in the vocabulary formed by the tf-idf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "train_tfidf_texts = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_tfidf_texts\n",
    "# adding information about the question number increases accurcy very little\n",
    "X = hstack((train_tfidf_texts, np.array(question_numbers)[:,None]))\n",
    "y = authorZipCode_regions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:51:53] /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[11:51:55] /workspace/src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 78 extra nodes, 0 pruned nodes, max_depth=6\n"
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "# param['silent'] = 1\n",
    "# param['verbosity'] = 2\n",
    "param['nthread'] = 8\n",
    "\n",
    "param['max_depth'] = 6\n",
    "num_boost_round = 2\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_boost_round = num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.40%\n",
      "Random accuracy: 1.02%\n"
     ]
    }
   ],
   "source": [
    "xgb_X_test = xgb.DMatrix(X_test)\n",
    "xgb_y_test = xgb.DMatrix(y_test)\n",
    "\n",
    "y_pred = bst.predict(xgb_X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "random_accuracy = 1 / len(set(authorZipCode_regions))\n",
    "print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "\n",
    "# fit_prior = False decreases acc by 0.1% which means that we have a slight imbalance between zipcodes\n",
    "nb_model = NB().fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 6.40%\n",
      "Random accuracy: 1.02%\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "random_accuracy = 1 / len(set(authorZipCode_regions))\n",
    "print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulated by regions (? of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names_to_zipCode_regions = {\n",
    "\"Île de France\": [\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"],\n",
    "\"Nord-Pas-de-Calais Picardie\": [\"02\", \"59\", \"60\", \"62\", \"80\"],\n",
    "\"Alsace Champagne-Ardenne Lorraine\": [\"08\", \"10\", \"51\", \"52\", \"54\", \"55\", \"57\", \"67\", \"88\", \"68\"],\n",
    "\"Normandie\": [\"14\", \"27\", \"50\", \"61\", \"76\"],\n",
    "\"Bretagne\": [\"22\", \"29\", \"35\", \"56\"],\n",
    "\"Pays de la Loire\": [\"44\", \"49\", \"53\", \"72\", \"85\"],\n",
    "\"Centre Val de Loire\": [\"18\", \"28\", \"36\", \"37\", \"41\", \"45\"],\n",
    "\"Bourgogne Franche-Comté\": [\"21\", \"25\", \"39\", \"58\", \"70\", \"71\", \"89\", \"90\"],\n",
    "\"Aquitaine Limousin Poitou-Charentes\": [\"16\", \"17\", \"19\", \"23\", \"24\", \"33\", \"40\", \"47\", \"64\", \"79\", \"86\", \"87\"],\n",
    "\"Auvergne-Rhône-Alpes\": [\"01\", \"03\", \"07\", \"15\", \"26\", \"38\", \"42\", \"43\", \"63\", \"69\", \"73\", \"74\"],\n",
    "\"Languedoc-Roussillon Midi Pyrénées\": [\"09\", \"11\", \"12\", \"30\", \"31\", \"32\", \"34\", \"46\", \"48\", \"65\", \"66\", \"81\", \"82\"],\n",
    "\"Provence Alpes Côte d'Azur\": [\"04\", \"05\", \"06\", \"13\", \"83\", \"84\"],\n",
    "\"Corse\": [\"20\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \" \".join(gensim.utils.simple_preprocess(text))\n",
    "    return text\n",
    "\n",
    "def strip_zipCode(zipCode):\n",
    "    return str(zipCode)[0:2]\n",
    "\n",
    "def get_region_by_stripped_zipCode(zipCode):\n",
    "    region_index = None\n",
    "    for idx, region in enumerate(region_names_to_zipCode_regions):\n",
    "        if zipCode in region_names_to_zipCode_regions[region]:\n",
    "            region_index = idx\n",
    "    return region_index\n",
    "\n",
    "def preprocess_zipCode(zipCode):\n",
    "    zipCode = strip_zipCode(zipCode)\n",
    "    return get_region_by_stripped_zipCode(zipCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the regions and answer text out of the dataframe, converting postal code to the region index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 0 / 65758\n",
      "row 10000 / 65758\n",
      "row 20000 / 65758\n",
      "row 30000 / 65758\n",
      "row 40000 / 65758\n",
      "row 50000 / 65758\n",
      "row 60000 / 65758\n",
      "zipCodes that does not belong to any region: {'3', '96', '5', '6', '97', '4', '99', '1', '9', '98', '7', '0'}\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "db = db_questions\n",
    "\n",
    "texts = []\n",
    "authorZipCode_regions = []\n",
    "# question_numbers = []\n",
    "i = 0\n",
    "invalid_zipcodes = []\n",
    "\n",
    "for row in range(db.shape[0]):\n",
    "    if row % 10000 == 0: print(\"row\", row, \"/\", db.shape[0])\n",
    "    for col in range(db.shape[1]):\n",
    "        if not pd.isnull(db.iat[row, col]):\n",
    "            text = preprocess_text(db.iat[row, col])\n",
    "            authorZipCode = db_raw.at[row, 'authorZipCode']\n",
    "            authorZipCode_region = preprocess_zipCode(authorZipCode)\n",
    "#             question_number = col\n",
    "            \n",
    "            if authorZipCode_region == None:\n",
    "                invalid_zipcodes.append(strip_zipCode(authorZipCode))\n",
    "                authorZipCode_region = -1\n",
    "\n",
    "            texts.append(text)\n",
    "            authorZipCode_regions.append(authorZipCode_region)\n",
    "#             question_numbers.append(question_number)\n",
    "\n",
    "invalid_zipcodes = set(invalid_zipcodes)\n",
    "print(\"zipCodes that does not belong to any region:\", invalid_zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_backup = texts.copy()\n",
    "authorZipCode_regions_backup = authorZipCode_regions.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exclude zipcodes that were not included to any of the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, zipCode in enumerate(authorZipCode_regions):\n",
    "    if zipCode == -1:\n",
    "        del texts[idx]\n",
    "        del authorZipCode_regions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# strip_accents = 'ascii' almost has not changed the model\n",
    "# limiting generated vocab size with max_features = ... decreases acc\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "train_tfidf_texts = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_tfidf_texts\n",
    "# X = train_tfidf_texts\n",
    "# adding information about the question number increases accurcy very little\n",
    "# X = hstack((train_tfidf_texts, np.array(question_numbers)[:,None]))\n",
    "y = authorZipCode_regions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "# param['silent'] = 1\n",
    "# param['verbosity'] = 2\n",
    "param['nthread'] = 4\n",
    "\n",
    "param['max_depth'] = 6\n",
    "num_boost_round = 10\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_boost_round = num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 5.66%\n",
      "Random accuracy: 7.14%\n"
     ]
    }
   ],
   "source": [
    "xgb_X_test = xgb.DMatrix(X_test)\n",
    "xgb_y_test = xgb.DMatrix(y_test)\n",
    "\n",
    "y_pred = bst.predict(xgb_X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "random_accuracy = 1 / len(set(authorZipCode_regions))\n",
    "print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as NB\n",
    "\n",
    "# fit_prior = False decreases acc by 0.1% which means that we have a slight imbalance between zipcodes\n",
    "nb_model = NB().fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 21.56%\n",
      "Random accuracy: 7.14%\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "random_accuracy = 1 / len(set(authorZipCode_regions))\n",
    "print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we can see, though doc2vec that can take the meaning of the sentence has performed poorly, TF-IDF and Naive Bayes gave pretty descent results  \n",
    "Naive Bayes seems to be naive (huh) but can actually handle sparse data greatly because it is robust to the missing values in data (and we have almost all the data missing because the word frequence vector for each example is very sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below allows to convert the texts = ['asdf', 'fdsa', ...] list with tags = [1, 0, 2, 1, 1, ..] to the texts and tags lists where the all the texts that correspond to the similar tag are joined together (it will be usefull in the following research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need it here, actually\n",
    "\n",
    "# thousand times slower\n",
    "# def join_text_with_same_tag(texts, tags):\n",
    "#     new_texts = []\n",
    "#     new_tags = []\n",
    "#     tag_to_text = {}\n",
    "#     for tag in set(tags):\n",
    "#         tag_to_text[tag] = \"\"\n",
    "    \n",
    "#     for idx, tag in enumerate(tags):\n",
    "# #         if idx % (len(tags) // 10) == 0: print(\"%.0f%% done\" % (idx / len(tags)))\n",
    "#         print_progress_bar(idx, len(tags))\n",
    "#         tag_to_text[tag] = \" \".join((tag_to_text[tag], texts[idx]))\n",
    "#     for tag in tag_to_text:\n",
    "#         new_texts.append(tag_to_text[tag])\n",
    "#         new_tags.append(tag)\n",
    "#     return new_texts, new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need it here, actually\n",
    "\n",
    "# def join_text_with_same_tag(texts, tags):\n",
    "#     new_texts = []\n",
    "#     new_tags = []\n",
    "#     tag_to_text = {}\n",
    "#     for tag in set(tags):\n",
    "#         tag_to_text[tag] = []\n",
    "    \n",
    "#     for idx, tag in enumerate(tags):\n",
    "#         if idx % (len(tags) // 10) == 0: print(\"%.0f%% done\" % (idx / len(tags) * 100))\n",
    "#         tag_to_text[tag].append(texts[idx])\n",
    "#     for tag in tag_to_text:\n",
    "#         tag_to_text[tag] = \" \".join(tag_to_text[tag])\n",
    "#     for tag in tag_to_text:\n",
    "#         new_texts.append(tag_to_text[tag])\n",
    "#         new_tags.append(tag)\n",
    "#     return new_texts, new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done\n",
      "10% done\n",
      "20% done\n",
      "30% done\n",
      "40% done\n",
      "50% done\n",
      "60% done\n",
      "70% done\n",
      "80% done\n",
      "90% done\n",
      "100% done\n",
      "CPU times: user 901 ms, sys: 72 ms, total: 973 ms\n",
      "Wall time: 972 ms\n"
     ]
    }
   ],
   "source": [
    "# We don't need it here, actually\n",
    "\n",
    "#%%time\n",
    "#region_joined_texts, region_joined_tags = join_text_with_same_tag(texts, authorZipCode_regions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
