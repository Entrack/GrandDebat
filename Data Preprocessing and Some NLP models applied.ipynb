{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "def find_and_set_sys_maxsize():\n",
    "    maxInt = sys.maxsize\n",
    "    decrement = True\n",
    "    while decrement:\n",
    "        decrement = False\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "            decrement = True\n",
    "find_and_set_sys_maxsize()\n",
    "\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import pickle\n",
    "import datetime\n",
    "def timestamp(): return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def print_progress(idx, final_idx, min_step = 1):\n",
    "    min_step = min_step if min_step <= final_idx else final_idx\n",
    "    if idx % (final_idx // 100 * min_step) == 0: print(\"%.0f%% done\" % (idx / final_idx * 100), end='\\r')\n",
    "    \n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import save_npz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import MultinomialNB as NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output_folder = './output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "granddebat_folder = '../GrandDébat/granddebat.fr/'\n",
    "democrat_path = os.path.join(granddebat_folder, 'DEMOCRATIE_ET_CITOYENNETE.csv')\n",
    "events_path = os.path.join(granddebat_folder, 'EVENTS.csv')\n",
    "fiscalit_path = os.path.join(granddebat_folder, 'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv')\n",
    "ecolog_path = os.path.join(granddebat_folder, 'LA_TRANSITION_ECOLOGIQUE.csv')\n",
    "organisat_path = os.path.join(granddebat_folder, 'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv')\n",
    "\n",
    "french_encoding = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = democrat_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_raw = pd.read_csv(file_path, engine='python', encoding=french_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>Ew</th>\n",
       "      <th>Ex</th>\n",
       "      <th>Ey</th>\n",
       "      <th>Ez</th>\n",
       "      <th>E0</th>\n",
       "      <th>E1</th>\n",
       "      <th>E2</th>\n",
       "      <th>...</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>Qx</th>\n",
       "      <th>Qy</th>\n",
       "      <th>Qz</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Le citoyen</td>\n",
       "      <td>Non</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Afin d’éviter de creuser les inégalités ne plu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un instrument de démocratie locale à modernise...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nous proposons le retour à la limitation de vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Voir l'intégralité de la proposition dans la d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POUR UN NOUVEAU CONTRAT CITOYEN               ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>député, maire, moi même</td>\n",
       "      <td>Non</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Budget participatif, possibilité d'interpeller...</td>\n",
       "      <td>Une bonne chose</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  A3   A4   A5  \\\n",
       "0                                         Le citoyen  Non  NaN   \n",
       "1  Un instrument de démocratie locale à modernise...  NaN  NaN   \n",
       "2                                                NaN  NaN  NaN   \n",
       "3  Voir l'intégralité de la proposition dans la d...  NaN  NaN   \n",
       "4                            député, maire, moi même  Non  NaN   \n",
       "\n",
       "                                                  Ew               Ex   Ey  \\\n",
       "0                                                NaN              NaN  NaN   \n",
       "1                                                NaN              NaN  NaN   \n",
       "2                                                NaN              NaN  NaN   \n",
       "3                                                NaN              NaN  NaN   \n",
       "4  Budget participatif, possibilité d'interpeller...  Une bonne chose  NaN   \n",
       "\n",
       "    Ez   E0   E1   E2                        ...                           M1  \\\n",
       "0  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "1  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "2  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "3  NaN  NaN  NaN  NaN                        ...                          NaN   \n",
       "4  NaN  Oui  NaN  NaN                        ...                          NaN   \n",
       "\n",
       "    M2   M3   M4   M5   Qx   Qy   Qz   Q0  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                                                  Q1  \n",
       "0  Afin d’éviter de creuser les inégalités ne plu...  \n",
       "1                                                NaN  \n",
       "2  Nous proposons le retour à la limitation de vi...  \n",
       "3  POUR UN NOUVEAU CONTRAT CITOYEN               ...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_reverse_index = 37\n",
    "db_questions = db_raw.loc[:, db_raw.columns[-questions_reverse_index:]]\n",
    "\n",
    "question_list = db_raw.columns[-questions_reverse_index:]\n",
    "question_list = [el.split()[0][-2:] for el in question_list]\n",
    "db_questions.columns = question_list\n",
    "\n",
    "db_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting raw X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = \" \".join(gensim.utils.simple_preprocess(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "db = db_questions\n",
    "\n",
    "texts = []\n",
    "authorZipCodes = []\n",
    "\n",
    "question_numbers = []\n",
    "\n",
    "for row in range(db.shape[0]):\n",
    "    print_progress(row, db.shape[0], 1)\n",
    "    for col in range(db.shape[1]):\n",
    "        if not pd.isnull(db.iat[row, col]):\n",
    "            text = preprocess_text(db.iat[row, col])\n",
    "            authorZipCode = db_raw.at[row, 'authorZipCode']\n",
    "            question_number = col\n",
    "\n",
    "            texts.append(text)\n",
    "            authorZipCodes.append(authorZipCode)\n",
    "            \n",
    "            question_numbers.append(question_number)\n",
    "authorZipCodes_np = np.array(authorZipCodes, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "db = db_questions\n",
    "\n",
    "texts = []\n",
    "authorZipCodes = []\n",
    "\n",
    "for row in range(db.shape[0]):\n",
    "    print_progress(row, db.shape[0], 1)\n",
    "    \n",
    "    author_texts = []\n",
    "    authorZipCode = db_raw.at[row, 'authorZipCode']\n",
    "    authorZipCodes.append(authorZipCode)\n",
    "    for col in range(db.shape[1]):\n",
    "        if not pd.isnull(db.iat[row, col]):\n",
    "            text = preprocess_text(db.iat[row, col])\n",
    "            author_texts.append(text)\n",
    "    texts.append(\" \".join(author_texts))\n",
    "            \n",
    "            \n",
    "authorZipCodes_np = np.array(authorZipCodes, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(results_output_folder, 'y_authorZipCodes.csv'), authorZipCodes_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal code regions (~100 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ZipCode_postal_region(zipCode):\n",
    "    return int(str(zipCode)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorZipCode_postal_regions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for ZipCode in authorZipCodes:\n",
    "    authorZipCode_postal_region = get_ZipCode_postal_region(ZipCode)\n",
    "    authorZipCode_postal_regions.append(authorZipCode_postal_region)\n",
    "\n",
    "authorZipCode_postal_regions = np.array(authorZipCode_postal_regions, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(results_output_folder, 'y_authorZipCode_postal_regions.csv'), authorZipCode_postal_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo regions (14 of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_names_to_zipCode_regions = {\n",
    "\"Île de France\": [\"75\", \"77\", \"78\", \"91\", \"92\", \"93\", \"94\", \"95\"],\n",
    "\"Nord-Pas-de-Calais Picardie\": [\"02\", \"59\", \"60\", \"62\", \"80\"],\n",
    "\"Alsace Champagne-Ardenne Lorraine\": [\"08\", \"10\", \"51\", \"52\", \"54\", \"55\", \"57\", \"67\", \"88\", \"68\"],\n",
    "\"Normandie\": [\"14\", \"27\", \"50\", \"61\", \"76\"],\n",
    "\"Bretagne\": [\"22\", \"29\", \"35\", \"56\"],\n",
    "\"Pays de la Loire\": [\"44\", \"49\", \"53\", \"72\", \"85\"],\n",
    "\"Centre Val de Loire\": [\"18\", \"28\", \"36\", \"37\", \"41\", \"45\"],\n",
    "\"Bourgogne Franche-Comté\": [\"21\", \"25\", \"39\", \"58\", \"70\", \"71\", \"89\", \"90\"],\n",
    "\"Aquitaine Limousin Poitou-Charentes\": [\"16\", \"17\", \"19\", \"23\", \"24\", \"33\", \"40\", \"47\", \"64\", \"79\", \"86\", \"87\"],\n",
    "\"Auvergne-Rhône-Alpes\": [\"01\", \"03\", \"07\", \"15\", \"26\", \"38\", \"42\", \"43\", \"63\", \"69\", \"73\", \"74\"],\n",
    "\"Languedoc-Roussillon Midi Pyrénées\": [\"09\", \"11\", \"12\", \"30\", \"31\", \"32\", \"34\", \"46\", \"48\", \"65\", \"66\", \"81\", \"82\"],\n",
    "\"Provence Alpes Côte d'Azur\": [\"04\", \"05\", \"06\", \"13\", \"83\", \"84\"],\n",
    "\"Corse\": [\"20\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_zipCode(zipCode):\n",
    "    return str(zipCode)[0:2]\n",
    "\n",
    "def get_region_by_stripped_zipCode(zipCode):\n",
    "    region_index = None\n",
    "    for idx, region in enumerate(region_names_to_zipCode_regions):\n",
    "        if zipCode in region_names_to_zipCode_regions[region]:\n",
    "            region_index = idx\n",
    "    return region_index\n",
    "\n",
    "def get_geo_region_by_ZipCode(zipCode):\n",
    "    zipCode = strip_zipCode(zipCode)\n",
    "    return get_region_by_stripped_zipCode(zipCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorZipCode_geo_regions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipCodes that does not belong to any region: {'9', '97', '3', '1', '99', '5', '98', '6', '7', '0', '96', '4'}\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "invalid_zipcodes = []\n",
    "for idx, ZipCode in enumerate(authorZipCodes):\n",
    "    print_progress(idx, len(authorZipCodes))\n",
    "    authorZipCode_geo_region = get_geo_region_by_ZipCode(ZipCode)\n",
    "    \n",
    "    if authorZipCode_geo_region == None:\n",
    "        authorZipCode_geo_region = -1\n",
    "        invalid_zipcodes.append(strip_zipCode(ZipCode))\n",
    "        \n",
    "    authorZipCode_geo_regions.append(authorZipCode_geo_region)\n",
    "\n",
    "authorZipCode_geo_regions = np.array(authorZipCode_geo_regions, dtype=np.int32)\n",
    "\n",
    "invalid_zipcodes = set(invalid_zipcodes)\n",
    "print(\"zipCodes that does not belong to any region:\", invalid_zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(results_output_folder, 'y_authorZipCode_geo_regions.csv'), authorZipCode_geo_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If city is big or not {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data_folder = '../data_base/'\n",
    "cities_data_path = os.path.join(cities_data_folder, 'correspondance-code-insee-code-postal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data = pd.read_csv('../data_base/correspondance-code-insee-code-postal.csv', \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`density_thresh` is a threshold to consider between big city and small city in terms of density of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_density = 117.48\n",
    "density_thresh = french_density * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this subregion is taken from the Data_set.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density_by_zip(zipCode, cities_data):\n",
    "    df = cities_data[cities_data['Code Postal'] == str(zipCode)]\n",
    "    \n",
    "    density = (df['Population'].sum() * 1000) / (df['Superficie'].sum() * 0.01)\n",
    "    if np.isnan(density):\n",
    "        density = 0\n",
    "        \n",
    "    return density\n",
    "\n",
    "def is_city_large(density, density_thresh):\n",
    "    return int(density > density_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_binary_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for idx, ZipCode in enumerate(authorZipCodes):\n",
    "    print_progress(idx, len(authorZipCodes))\n",
    "    if_city_is_large = is_city_large(get_density_by_zip(ZipCode, cities_data), density_thresh)\n",
    "    city_binary_sizes.append(if_city_is_large)\n",
    "city_binary_sizes = np.array(city_binary_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cities_data_cache = {}\n",
    "for idx, ZipCode in enumerate(authorZipCodes):\n",
    "    print_progress(idx, len(authorZipCodes))\n",
    "    if not ZipCode in cities_data_cache:       \n",
    "        df = cities_data[cities_data['Code Postal'] == str(ZipCode)]\n",
    "        density = (df['Population'].sum() * 1000) / (df['Superficie'].sum() * 0.01)\n",
    "        cities_data_cache[ZipCode] = density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_density_by_zip(zipCode, cities_data_cache):\n",
    "    return cities_data_cache[zipCode]\n",
    "\n",
    "def is_city_large(density, density_thresh):\n",
    "    return int(density > density_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_binary_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for idx, ZipCode in enumerate(authorZipCodes):\n",
    "    print_progress(idx, len(authorZipCodes))\n",
    "    if_city_is_large = is_city_large(get_density_by_zip(ZipCode, cities_data_cache), density_thresh)\n",
    "    city_binary_sizes.append(if_city_is_large)\n",
    "city_binary_sizes = np.array(city_binary_sizes, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(results_output_folder, 'y_city_binary_sizes.csv'), city_binary_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X from simple NLP models application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 1)\n",
    "X_tfidf = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(os.path.join(results_output_folder, 'X_tfidf.npz'), X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do load_npz by `from scipy.sparse import load_npz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec + PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kyubyong/wordvectors  \n",
    "French (w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_pretrained_folder = './pretrained_w2v/'\n",
    "w2vec_pretrained_path = os.path.join(w2vec_pretrained_folder, 'fr.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(w2vec_pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_texts_vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% done\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unknown_to_pretrained_word_list = []\n",
    "dtype = np.float32\n",
    "for idx, text in enumerate(texts):\n",
    "    print_progress(idx, len(texts))\n",
    "    \n",
    "    word_vectors = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            word_vectors.append(w2v_model[word])\n",
    "        except:\n",
    "            if not word in unknown_to_pretrained_word_list:\n",
    "                unknown_to_pretrained_word_list.append(word)\n",
    "\n",
    "    if not len(word_vectors) == 0:\n",
    "        word_vectors = np.array(word_vectors, dtype = dtype)\n",
    "        words_mean_vector = word_vectors.sum(axis = 0) / word_vectors.shape[0]\n",
    "    else:\n",
    "        words_mean_vector = np.zeros((300,), dtype = dtype)\n",
    "        \n",
    "    w2vec_texts_vectors.append(words_mean_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_texts_vectors_np = np.vstack(w2vec_texts_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2vec_texts_vectors_np = np.array(w2vec_texts_vectors, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2vec_texts_vectors_np -= w2vec_texts_vectors_np.min()\n",
    "# w2vec_texts_vectors_np /= w2vec_texts_vectors_np.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably get a MemoryError if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pca = PCA(n_components = n_components)\n",
    "X_w2vec_pca = pca.fit_transform(w2vec_texts_vectors_np) # w2vec_texts_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_w2vec_pca_%d_components.csv' % (n_components)\n",
    "np.savetxt(os.path.join(results_output_folder, filename), X_w2vec_pca, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can perform somewhat similar but without MemoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, algorithm='arpack')\n",
    "X_w2vec_svd = svd.fit_transform(w2vec_texts_vectors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'X_w2vec_svd_%d_components.csv' % (n_components)\n",
    "np.savetxt(os.path.join(results_output_folder, filename), X_w2vec_svd, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_accuracy(model, X_test, y_test, y):\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Model accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    random_accuracy = 1 / len(set(y))\n",
    "    print(\"Random accuracy: %.2f%%\" % (random_accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_tfidf\n",
    "y = authorZipCode_geo_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param = {}\n",
    "param['nthread'] = 4\n",
    "param['max_depth'] = 6\n",
    "num_boost_round = 10\n",
    "\n",
    "xgb_model = xgb.train(param, dtrain, num_boost_round = num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 5.62%\n",
      "Random accuracy: 7.14%\n"
     ]
    }
   ],
   "source": [
    "xgb_X_test = xgb.DMatrix(X_test)\n",
    "get_model_accuracy(xgb_model, xgb_X_test, y_test, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_prior = False` decreases acc by 0.1% which means that we have a slight imbalance between zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nb_model = NB().fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 21.43%\n",
      "Random accuracy: 7.14%\n"
     ]
    }
   ],
   "source": [
    "get_model_accuracy(nb_model, X_test, y_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
